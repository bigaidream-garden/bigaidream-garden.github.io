<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep-learning on Jethro&#39;s Braindump</title>
    <link>https://bigaidream.github.io/mind-garden/tags/deep-learning/</link>
    <description>Recent content in deep-learning on Jethro&#39;s Braindump</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Jul 2020 00:56:39 +0800</lastBuildDate><atom:link href="https://bigaidream.github.io/mind-garden/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bayesian Deep Learning</title>
      <link>https://bigaidream.github.io/mind-garden/posts/bayesian_deep_learning/</link>
      <pubDate>Fri, 17 Jul 2020 00:56:39 +0800</pubDate>
      
      <guid>https://bigaidream.github.io/mind-garden/posts/bayesian_deep_learning/</guid>
      <description>The Case For Bayesian Learning (Wilson, n.d.)  Vague parameter prior + structured model (e.g. CNN) = structured function prior! The success of ensembles encourages Bayesians, since ensembles approximate the Bayesian Model Average  Bayesian Perspective on Generalization (Smith and Le, n.d.) Bayesian model comparisons were first made on Neural Networks by Mackay. Consider a classification model \(M\) with a single parameter \(w\), training inputs \(x\) and training labels \(y\). We can infer a posterior probability distribution over the parameter by applying Bayes theorem:</description>
    </item>
    
  </channel>
</rss>
